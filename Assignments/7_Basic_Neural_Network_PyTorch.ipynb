{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5ab5479",
   "metadata": {},
   "source": [
    "\n",
    "# Basic Neural Network with PyTorch\n",
    "\n",
    "This notebook provides a step-by-step guide to building a basic neural network using PyTorch. We'll use the Breast Cancer dataset from Scikit-Learn to demonstrate a binary classification task.\n",
    "\n",
    "By the end of this notebook, you'll understand how to:\n",
    "\n",
    "1. Load and preprocess data for a neural network\n",
    "2. Define and train a neural network using PyTorch\n",
    "3. Evaluate and make predictions with the trained model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f065ea8b",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa696471",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598364b6",
   "metadata": {},
   "source": [
    "## Understanding the Breast Cancer Dataset\n",
    "\n",
    "In this example, we're using the **Breast Cancer dataset** from Scikit-Learn, a well-known dataset for binary classification. This dataset contains information derived from digitized images of breast cancer tissue samples. Specifically, it focuses on **30 features** that describe the characteristics of cell nuclei within these samples. The goal is to predict whether a tumor is **malignant** (cancerous) or **benign** (non-cancerous).\n",
    "\n",
    "### Breakdown of the Dataset\n",
    "\n",
    "1. **Features (Inputs)**:\n",
    "   - Each sample (or row) represents a breast cancer tissue sample.\n",
    "   - Each sample includes 30 features, which describe cell nuclei measurements, such as:\n",
    "     - **Radius**: Mean radius of the cell nuclei.\n",
    "     - **Texture**: Variation in cell texture.\n",
    "     - **Perimeter** and **Area**: Measurements that give insights into cell size.\n",
    "     - **Smoothness**: Describes how smooth the cell boundaries are.\n",
    "     - **Compactness**, **Concavity**, and **Concave points**: Shape-related metrics.\n",
    "     - **Symmetry** and **Fractal dimension**: Provide structural information.\n",
    "\n",
    "   These features help the model learn patterns that distinguish between malignant and benign tumors.\n",
    "\n",
    "2. **Target (Output)**:\n",
    "   - The target variable (label) is binary, with two possible values:\n",
    "     - **0** for benign (non-cancerous).\n",
    "     - **1** for malignant (cancerous).\n",
    "   - Our model learns to predict this label based on the input features, identifying patterns that distinguish benign from malignant tumors.\n",
    "\n",
    "### Example Data Sample\n",
    "\n",
    "Each data sample might look something like this (simplified for clarity):\n",
    "\n",
    "| Radius | Texture | Perimeter | Area | Smoothness | Compactness | Concavity | ... | Target |\n",
    "|--------|---------|-----------|------|------------|-------------|-----------|-----|--------|\n",
    "| 17.99  | 10.38   | 122.8     | 1001 | 0.1184     | 0.2776      | 0.3001    | ... | 1 (Malignant) |\n",
    "| 13.54  | 14.36   | 87.46     | 566  | 0.09779    | 0.08129     | 0.06664   | ... | 0 (Benign)    |\n",
    "\n",
    "Each value in the columns represents a measurement from a cell image. The model uses these values to classify new samples as either malignant or benign.\n",
    "\n",
    "### Why This Data?\n",
    "\n",
    "The Breast Cancer dataset is widely used because:\n",
    "- It has real-world medical relevance, making it useful for applications in healthcare.\n",
    "- It’s small enough for efficient training, ideal for demonstrating machine learning concepts.\n",
    "- It provides a clear binary classification task, making it easy to interpret the network's output and evaluate model performance.\n",
    "\n",
    "### Summary\n",
    "\n",
    "This dataset allows us to create a model that learns patterns in cell measurements to predict if a tumor is cancerous. By training on these features and labels, our model can generalize well enough to classify new, unseen samples. This is a valuable capability in real-world medical applications, where accurate predictions can assist in diagnosis and treatment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c36c671e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Description:\n",
      "\n",
      ".. _breast_cancer_dataset:\n",
      "\n",
      "Breast cancer wisconsin (diagnostic) dataset\n",
      "--------------------------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      ":Number of Instances: 569\n",
      "\n",
      ":Number of Attributes: 30 numeric, predictive attributes and the class\n",
      "\n",
      ":Attribute Information:\n",
      "    - radius (mean of distances from center to points on the perimeter)\n",
      "    - texture (standard deviation of gray-scale values)\n",
      "    - perimeter\n",
      "    - area\n",
      "    - smoothness (local variation in radius lengths)\n",
      "    - compactness (perimeter^2 / area - 1.0)\n",
      "    - concavity (severity of concave portions of the contour)\n",
      "    - concave points (number of concave portions of the contour)\n",
      "    - symmetry\n",
      "    - fractal dimension (\"coastline approximation\" - 1)\n",
      "\n",
      "    The mean, standard error, and \"worst\" or largest (mean of the three\n",
      "    worst/largest values) of these features were computed for each image,\n",
      "    resulting in 30 features.  For instance, field 0 is Mean Radius, field\n",
      "    10 is Radius SE, field 20 is Worst Radius.\n",
      "\n",
      "    - class:\n",
      "            - WDBC-Malignant\n",
      "            - WDBC-Benign\n",
      "\n",
      ":Summary Statistics:\n",
      "\n",
      "===================================== ====== ======\n",
      "                                        Min    Max\n",
      "===================================== ====== ======\n",
      "radius (mean):                        6.981  28.11\n",
      "texture (mean):                       9.71   39.28\n",
      "perimeter (mean):                     43.79  188.5\n",
      "area (mean):                          143.5  2501.0\n",
      "smoothness (mean):                    0.053  0.163\n",
      "compactness (mean):                   0.019  0.345\n",
      "concavity (mean):                     0.0    0.427\n",
      "concave points (mean):                0.0    0.201\n",
      "symmetry (mean):                      0.106  0.304\n",
      "fractal dimension (mean):             0.05   0.097\n",
      "radius (standard error):              0.112  2.873\n",
      "texture (standard error):             0.36   4.885\n",
      "perimeter (standard error):           0.757  21.98\n",
      "area (standard error):                6.802  542.2\n",
      "smoothness (standard error):          0.002  0.031\n",
      "compactness (standard error):         0.002  0.135\n",
      "concavity (standard error):           0.0    0.396\n",
      "concave points (standard error):      0.0    0.053\n",
      "symmetry (standard error):            0.008  0.079\n",
      "fractal dimension (standard error):   0.001  0.03\n",
      "radius (worst):                       7.93   36.04\n",
      "texture (worst):                      12.02  49.54\n",
      "perimeter (worst):                    50.41  251.2\n",
      "area (worst):                         185.2  4254.0\n",
      "smoothness (worst):                   0.071  0.223\n",
      "compactness (worst):                  0.027  1.058\n",
      "concavity (worst):                    0.0    1.252\n",
      "concave points (worst):               0.0    0.291\n",
      "symmetry (worst):                     0.156  0.664\n",
      "fractal dimension (worst):            0.055  0.208\n",
      "===================================== ====== ======\n",
      "\n",
      ":Missing Attribute Values: None\n",
      "\n",
      ":Class Distribution: 212 - Malignant, 357 - Benign\n",
      "\n",
      ":Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n",
      "\n",
      ":Donor: Nick Street\n",
      "\n",
      ":Date: November, 1995\n",
      "\n",
      "This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\n",
      "https://goo.gl/U2Uwz2\n",
      "\n",
      "Features are computed from a digitized image of a fine needle\n",
      "aspirate (FNA) of a breast mass.  They describe\n",
      "characteristics of the cell nuclei present in the image.\n",
      "\n",
      "Separating plane described above was obtained using\n",
      "Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\n",
      "Construction Via Linear Programming.\" Proceedings of the 4th\n",
      "Midwest Artificial Intelligence and Cognitive Science Society,\n",
      "pp. 97-101, 1992], a classification method which uses linear\n",
      "programming to construct a decision tree.  Relevant features\n",
      "were selected using an exhaustive search in the space of 1-4\n",
      "features and 1-3 separating planes.\n",
      "\n",
      "The actual linear program used to obtain the separating plane\n",
      "in the 3-dimensional space is that described in:\n",
      "[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\n",
      "Programming Discrimination of Two Linearly Inseparable Sets\",\n",
      "Optimization Methods and Software 1, 1992, 23-34].\n",
      "\n",
      "This database is also available through the UW CS ftp server:\n",
      "\n",
      "ftp ftp.cs.wisc.edu\n",
      "cd math-prog/cpo-dataset/machine-learn/WDBC/\n",
      "\n",
      ".. dropdown:: References\n",
      "\n",
      "  - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction\n",
      "    for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on\n",
      "    Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\n",
      "    San Jose, CA, 1993.\n",
      "  - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and\n",
      "    prognosis via linear programming. Operations Research, 43(4), pages 570-577,\n",
      "    July-August 1995.\n",
      "  - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\n",
      "    to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994)\n",
      "    163-171.\n",
      "\n",
      "\n",
      "First Few Rows of the Dataset:\n",
      "\n",
      "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
      "0        17.99         10.38          122.80     1001.0          0.11840   \n",
      "1        20.57         17.77          132.90     1326.0          0.08474   \n",
      "2        19.69         21.25          130.00     1203.0          0.10960   \n",
      "3        11.42         20.38           77.58      386.1          0.14250   \n",
      "4        20.29         14.34          135.10     1297.0          0.10030   \n",
      "\n",
      "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
      "0           0.27760          0.3001              0.14710         0.2419   \n",
      "1           0.07864          0.0869              0.07017         0.1812   \n",
      "2           0.15990          0.1974              0.12790         0.2069   \n",
      "3           0.28390          0.2414              0.10520         0.2597   \n",
      "4           0.13280          0.1980              0.10430         0.1809   \n",
      "\n",
      "   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
      "0                 0.07871  ...          17.33           184.60      2019.0   \n",
      "1                 0.05667  ...          23.41           158.80      1956.0   \n",
      "2                 0.05999  ...          25.53           152.50      1709.0   \n",
      "3                 0.09744  ...          26.50            98.87       567.7   \n",
      "4                 0.05883  ...          16.67           152.20      1575.0   \n",
      "\n",
      "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
      "0            0.1622             0.6656           0.7119                0.2654   \n",
      "1            0.1238             0.1866           0.2416                0.1860   \n",
      "2            0.1444             0.4245           0.4504                0.2430   \n",
      "3            0.2098             0.8663           0.6869                0.2575   \n",
      "4            0.1374             0.2050           0.4000                0.1625   \n",
      "\n",
      "   worst symmetry  worst fractal dimension  target  \n",
      "0          0.4601                  0.11890       0  \n",
      "1          0.2750                  0.08902       0  \n",
      "2          0.3613                  0.08758       0  \n",
      "3          0.6638                  0.17300       0  \n",
      "4          0.2364                  0.07678       0  \n",
      "\n",
      "[5 rows x 31 columns]\n",
      "\n",
      "Summary Statistics:\n",
      "\n",
      "       mean radius  mean texture  mean perimeter    mean area  \\\n",
      "count   569.000000    569.000000      569.000000   569.000000   \n",
      "mean     14.127292     19.289649       91.969033   654.889104   \n",
      "std       3.524049      4.301036       24.298981   351.914129   \n",
      "min       6.981000      9.710000       43.790000   143.500000   \n",
      "25%      11.700000     16.170000       75.170000   420.300000   \n",
      "50%      13.370000     18.840000       86.240000   551.100000   \n",
      "75%      15.780000     21.800000      104.100000   782.700000   \n",
      "max      28.110000     39.280000      188.500000  2501.000000   \n",
      "\n",
      "       mean smoothness  mean compactness  mean concavity  mean concave points  \\\n",
      "count       569.000000        569.000000      569.000000           569.000000   \n",
      "mean          0.096360          0.104341        0.088799             0.048919   \n",
      "std           0.014064          0.052813        0.079720             0.038803   \n",
      "min           0.052630          0.019380        0.000000             0.000000   \n",
      "25%           0.086370          0.064920        0.029560             0.020310   \n",
      "50%           0.095870          0.092630        0.061540             0.033500   \n",
      "75%           0.105300          0.130400        0.130700             0.074000   \n",
      "max           0.163400          0.345400        0.426800             0.201200   \n",
      "\n",
      "       mean symmetry  mean fractal dimension  ...  worst texture  \\\n",
      "count     569.000000              569.000000  ...     569.000000   \n",
      "mean        0.181162                0.062798  ...      25.677223   \n",
      "std         0.027414                0.007060  ...       6.146258   \n",
      "min         0.106000                0.049960  ...      12.020000   \n",
      "25%         0.161900                0.057700  ...      21.080000   \n",
      "50%         0.179200                0.061540  ...      25.410000   \n",
      "75%         0.195700                0.066120  ...      29.720000   \n",
      "max         0.304000                0.097440  ...      49.540000   \n",
      "\n",
      "       worst perimeter   worst area  worst smoothness  worst compactness  \\\n",
      "count       569.000000   569.000000        569.000000         569.000000   \n",
      "mean        107.261213   880.583128          0.132369           0.254265   \n",
      "std          33.602542   569.356993          0.022832           0.157336   \n",
      "min          50.410000   185.200000          0.071170           0.027290   \n",
      "25%          84.110000   515.300000          0.116600           0.147200   \n",
      "50%          97.660000   686.500000          0.131300           0.211900   \n",
      "75%         125.400000  1084.000000          0.146000           0.339100   \n",
      "max         251.200000  4254.000000          0.222600           1.058000   \n",
      "\n",
      "       worst concavity  worst concave points  worst symmetry  \\\n",
      "count       569.000000            569.000000      569.000000   \n",
      "mean          0.272188              0.114606        0.290076   \n",
      "std           0.208624              0.065732        0.061867   \n",
      "min           0.000000              0.000000        0.156500   \n",
      "25%           0.114500              0.064930        0.250400   \n",
      "50%           0.226700              0.099930        0.282200   \n",
      "75%           0.382900              0.161400        0.317900   \n",
      "max           1.252000              0.291000        0.663800   \n",
      "\n",
      "       worst fractal dimension      target  \n",
      "count               569.000000  569.000000  \n",
      "mean                  0.083946    0.627417  \n",
      "std                   0.018061    0.483918  \n",
      "min                   0.055040    0.000000  \n",
      "25%                   0.071460    0.000000  \n",
      "50%                   0.080040    1.000000  \n",
      "75%                   0.092080    1.000000  \n",
      "max                   0.207500    1.000000  \n",
      "\n",
      "[8 rows x 31 columns]\n",
      "\n",
      "Target Distribution:\n",
      "\n",
      "target\n",
      "1    357\n",
      "0    212\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Feature Names:\n",
      " ['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
      " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
      " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
      " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
      " 'smoothness error' 'compactness error' 'concavity error'\n",
      " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
      " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
      " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
      " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n",
      "\n",
      "Target Names:\n",
      " ['malignant' 'benign']\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Breast Cancer dataset\n",
    "data = load_breast_cancer()\n",
    "\n",
    "# Print dataset description\n",
    "print(\"Dataset Description:\\n\")\n",
    "print(data.DESCR)\n",
    "\n",
    "# Convert the dataset to a DataFrame for easier viewing\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['target'] = data.target  # Add the target column\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(\"\\nFirst Few Rows of the Dataset:\\n\")\n",
    "print(df.head())\n",
    "\n",
    "# Display summary statistics for the DataFrame\n",
    "print(\"\\nSummary Statistics:\\n\")\n",
    "print(df.describe())\n",
    "\n",
    "# Display target distribution\n",
    "print(\"\\nTarget Distribution:\\n\")\n",
    "print(df['target'].value_counts())\n",
    "\n",
    "# Print feature and target names for reference\n",
    "print(\"\\nFeature Names:\\n\", data.feature_names)\n",
    "print(\"\\nTarget Names:\\n\", data.target_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4b5bb2",
   "metadata": {},
   "source": [
    "\n",
    "## Step 2: Load and Prepare the Data\n",
    "\n",
    "In this step, we load the breast cancer dataset, split it into training and testing sets, and normalize it for faster convergence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "840e7159",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03125141",
   "metadata": {},
   "source": [
    "### Code Explanation: Preparing the Breast Cancer Dataset for Neural Network Training\n",
    "\n",
    "The following code prepares the **Breast Cancer dataset** for training a neural network by following essential steps: data loading, splitting, normalization, and conversion to PyTorch tensors.\n",
    "\n",
    "1. **Load Dataset**:\n",
    "   - The function `load_breast_cancer()` loads the Breast Cancer dataset from Scikit-Learn, returning a special data structure called a **Bunch**.\n",
    "   - `data.data`: This contains the feature values for each sample in the dataset. It’s a 2D array with shape `(samples, features)`, where each row is a sample, and each column is a feature (like mean radius or smoothness).\n",
    "   - `data.target`: This contains the labels or targets (0 for benign and 1 for malignant) associated with each sample.\n",
    "   \n",
    "   **Variables Created**:\n",
    "   - `X`: Stores the features (predictors) from the dataset.\n",
    "   - `y`: Stores the target labels.\n",
    "\n",
    "---\n",
    "\n",
    "2. **Split the Data**:\n",
    "   - `train_test_split(X, y, test_size=0.2, random_state=42)`: This function from Scikit-Learn splits the data into **training** and **testing** sets.\n",
    "     - `test_size=0.2`: Reserves 20% of the data for testing and 80% for training.\n",
    "     - `random_state=42`: Ensures reproducibility of the split (the same data split each time).\n",
    "   - **Why Split?** The model is trained on the training set and evaluated on the test set to check its generalization to unseen data.\n",
    "\n",
    "   **Variables Created**:\n",
    "   - `X_train` and `y_train`: The features and labels for the training set.\n",
    "   - `X_test` and `y_test`: The features and labels for the test set.\n",
    "\n",
    "---\n",
    "\n",
    "3. **Normalize the Data**:\n",
    "   - Neural networks perform better when the input data is **normalized** (scaled to a standard range).\n",
    "   - `StandardScaler()`: Scales each feature to have a mean of 0 and a standard deviation of 1.\n",
    "     - `scaler.fit_transform(X_train)`: Fits the scaler to `X_train` and transforms the data.\n",
    "     - `scaler.transform(X_test)`: Transforms `X_test` using the same scaling parameters from `X_train` to maintain consistency.\n",
    "\n",
    "   **Variables Updated**:\n",
    "   - `X_train` and `X_test` are now scaled, improving the model’s performance and helping the neural network converge faster during training.\n",
    "\n",
    "---\n",
    "\n",
    "4. **Convert Data to PyTorch Tensors**:\n",
    "   - PyTorch models require data to be in **tensor** format.\n",
    "   - `torch.tensor(data, dtype=torch.float32)`: Converts `X_train`, `y_train`, `X_test`, and `y_test` to PyTorch tensors with a `float32` data type, the default format for neural network inputs.\n",
    "   - The `float32` type is essential for efficient model training, while `float64` (double precision) is not necessary and can slow down computation.\n",
    "\n",
    "   **Variables Created**:\n",
    "   - `X_train_tensor`, `y_train_tensor`, `X_test_tensor`, and `y_test_tensor`: These PyTorch tensors are the input data prepared and ready for training a neural network model.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "\n",
    "These steps ensure that:\n",
    "1. The data is split into training and test sets to allow model evaluation.\n",
    "2. Features are scaled to a standard range, making training more efficient.\n",
    "3. Data is converted to tensors for compatibility with PyTorch models, setting the stage for model training.\n",
    "\n",
    "This preparation is critical for creating a robust and efficient neural network training pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3dfada",
   "metadata": {},
   "source": [
    "\n",
    "## Step 3: Create DataLoader for Batch Processing\n",
    "\n",
    "Using DataLoader allows us to load data in batches, which helps in efficient training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26d44992",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create TensorDataset and DataLoader\n",
    "train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "\n",
    "test_data = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acad5305",
   "metadata": {},
   "source": [
    "This code snippet prepares the data for efficient training and evaluation of the neural network by creating **TensorDataset** and **DataLoader** objects. These tools are useful for managing and feeding data into the neural network during training and testing in PyTorch.\n",
    "\n",
    "### Explanation of Each Step\n",
    "\n",
    "1. **TensorDataset**:\n",
    "   - `TensorDataset` is a PyTorch utility that groups input data (`X_train_tensor` and `X_test_tensor`) and their corresponding target labels (`y_train_tensor` and `y_test_tensor`) together.\n",
    "   - By creating `TensorDataset` objects (`train_data` and `test_data`), we combine each feature set (input) with its label (target), making it easier to manage and feed to the model during training and testing.\n",
    "   - `train_data` contains all training samples and their labels, while `test_data` contains all test samples and labels.\n",
    "\n",
    "   **Example**:\n",
    "   - `train_data[0]` would return a tuple containing the first training sample and its corresponding label.\n",
    "\n",
    "2. **DataLoader**:\n",
    "   - `DataLoader` takes a dataset (like `train_data` or `test_data`) and provides an efficient way to iterate over the data in **batches**.\n",
    "   - It loads data in **batches** of a specified size (`batch_size=32` in this example), meaning each iteration of the DataLoader will yield 32 samples at a time. This batch processing is more memory efficient and speeds up training.\n",
    "   - **`shuffle=True`** in `train_loader`: Randomly shuffles the data before each epoch, which helps prevent the model from learning any order-based patterns and generally leads to better model generalization.\n",
    "   - **`shuffle=False`** in `test_loader`: The test set doesn’t need shuffling because it is only used for evaluation, not learning. Keeping it in the same order allows for consistent evaluation.\n",
    "\n",
    "3. **Batch Size**:\n",
    "   - **Batch Size** (`batch_size=32`): Refers to the number of samples the model processes before updating the weights. A batch size of 32 is common, balancing training speed and memory efficiency.\n",
    "   - When `DataLoader` iterates, it will yield 32 samples at a time, allowing the model to calculate the gradients and update weights based on each batch rather than the entire dataset.\n",
    "\n",
    "### Summary\n",
    "- `train_loader` and `test_loader` now allow the model to iterate over the training and test datasets in efficient batches, which helps with:\n",
    "  - **Faster training**: Processing small batches of data instead of the entire dataset at once.\n",
    "  - **Memory efficiency**: By not loading the entire dataset at once.\n",
    "  - **Shuffling**: Improving model generalization for training batches (only applied to the training set).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b10d516",
   "metadata": {},
   "source": [
    "\n",
    "## Step 4: Define the Neural Network Model\n",
    "\n",
    "We'll define a simple neural network with a single hidden layer. We'll use ReLU activation in the hidden layer and Sigmoid activation in the output layer for binary classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "597d5c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.hidden = nn.Linear(input_size, 16)  # Hidden layer with 16 neurons\n",
    "        self.output = nn.Linear(16, 1)           # Output layer for binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.hidden(x))           # ReLU activation for hidden layer\n",
    "        x = torch.sigmoid(self.output(x))        # Sigmoid activation for output layer\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34f6c50",
   "metadata": {},
   "source": [
    "This code defines a simple neural network in PyTorch. Let's break down each part to understand how it works.\n",
    "\n",
    "### Class Definition: `SimpleNN`\n",
    "\n",
    "The `SimpleNN` class inherits from `nn.Module`, which is the base class for all neural network modules in PyTorch. This inheritance allows us to define custom architectures and layers, making it easy to structure and extend neural networks.\n",
    "\n",
    "1. **`__init__` Method (Initialization)**:\n",
    "   - **Purpose**: Defines the layers of the neural network. In this case, the network has one hidden layer and one output layer.\n",
    "   - **Parameters**:\n",
    "     - `input_size`: The number of input features in each sample. This allows the network to accept input data with a specific number of features.\n",
    "   - **Layers**:\n",
    "     - `self.hidden`: Creates a **hidden layer** with `input_size` inputs and 16 neurons (outputs). The `nn.Linear` layer performs a linear transformation \\( xW + b \\), where `x` is the input, `W` is the weight matrix, and `b` is the bias.\n",
    "     - `self.output`: Creates an **output layer** with 16 inputs (matching the hidden layer’s outputs) and 1 neuron. The single neuron is suitable for **binary classification**, outputting a single probability.\n",
    "\n",
    "### `forward` Method (Forward Pass)\n",
    "\n",
    "The `forward` method defines how data flows through the network. Each layer applies transformations to the data as it passes through.\n",
    "\n",
    "2. **Activation Functions**:\n",
    "   - **ReLU Activation (`torch.relu`)**:\n",
    "     - Applied to the hidden layer. ReLU (Rectified Linear Unit) sets negative values to zero and keeps positive values the same, introducing non-linearity and helping the network learn complex patterns.\n",
    "     - This operation takes the output of `self.hidden(x)` and applies ReLU, which is crucial for deep networks to prevent them from behaving like linear models.\n",
    "   - **Sigmoid Activation (`torch.sigmoid`)**:\n",
    "     - Applied to the output layer. Sigmoid squashes the output to a range between 0 and 1, making it interpretable as a probability in binary classification tasks.\n",
    "     - This means that for each input sample, the output is a probability between 0 (class 0) and 1 (class 1).\n",
    "\n",
    "3. **Return Statement**:\n",
    "   - The function returns the output from the sigmoid layer, which represents the probability that the input belongs to the positive class (e.g., class 1).\n",
    "\n",
    "### Summary\n",
    "\n",
    "This network structure is straightforward:\n",
    "- **Input Layer** → **Hidden Layer** with 16 neurons → **Output Layer** with 1 neuron.\n",
    "- It uses **ReLU** for the hidden layer and **Sigmoid** for the output, making it ideal for binary classification.\n",
    "- By defining layers and activation functions in the `__init__` and `forward` methods, PyTorch handles the backward propagation automatically, allowing easy training and optimization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fea88c2",
   "metadata": {},
   "source": [
    "\n",
    "## Step 5: Instantiate Model, Loss Function, and Optimizer\n",
    "\n",
    "We initialize the model, specify binary cross-entropy as the loss function, and use the Adam optimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3be2b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_size = X_train.shape[1]  # Number of features in the dataset\n",
    "model = SimpleNN(input_size)\n",
    "\n",
    "criterion = nn.BCELoss()  # Binary cross-entropy loss for binary classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c576e2d",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "\n",
    "#### Determining `input_size`:\n",
    "\n",
    "- `input_size = X_train.shape[1]`: Sets `input_size` to the number of features in the training dataset.\n",
    "- `X_train.shape[1]` gives the number of columns (features) in `X_train`, which represents the number of input features the model will need.\n",
    "- This value is passed to the `SimpleNN` model so the input layer matches the number of features in each sample.\n",
    "\n",
    "#### Initializing the Model:\n",
    "\n",
    "- `model = SimpleNN(input_size)`: Instantiates the neural network model using the `SimpleNN` class, with `input_size` as an argument.\n",
    "- This line creates the model object (`model`) and defines its architecture based on `SimpleNN`, with layers and activations that we defined previously.\n",
    "\n",
    "#### Setting the Loss Function (`criterion`):\n",
    "\n",
    "- `criterion = nn.BCELoss()`: Defines the Binary Cross-Entropy Loss as the loss function.\n",
    "- **Binary Cross-Entropy (BCE) Loss**: A common loss function for binary classification tasks, calculating the difference between the predicted probabilities and actual labels.\n",
    "- BCE loss penalizes incorrect predictions more heavily, helping the model to learn better classifications between two classes (e.g., benign vs. malignant).\n",
    "\n",
    "#### Choosing the Optimizer (`optimizer`):\n",
    "\n",
    "- `optimizer = optim.Adam(model.parameters(), lr=0.001)`: Initializes the Adam optimizer with a learning rate of `0.001`.\n",
    "- **Adam (Adaptive Moment Estimation)**: A popular optimizer that adjusts the learning rate during training, making it highly efficient and well-suited for deep learning.\n",
    "- `model.parameters()`: Passes all parameters (weights and biases) of `model` to the optimizer, so Adam can adjust them to minimize the loss.\n",
    "- **Learning Rate (`lr=0.001`)**: Specifies how fast or slow the optimizer updates model parameters. A smaller learning rate allows for gradual adjustments, which can improve accuracy.\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **Model Initialization**: `SimpleNN` is instantiated with the correct input size based on the number of features.\n",
    "- **Loss Function (`criterion`)**: Binary Cross-Entropy Loss is chosen for binary classification.\n",
    "- **Optimizer (`optimizer`)**: Adam optimizer is initialized with a learning rate of `0.001`, making it adaptive for efficient training.\n",
    "\n",
    "This setup ensures that the model, loss function, and optimizer are configured and ready for training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a91c9ac",
   "metadata": {},
   "source": [
    "\n",
    "## Step 6: Train the Model\n",
    "\n",
    "Here we train the model for 10 epochs. During each epoch, we calculate the forward pass, compute the loss, perform the backward pass, and update the weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7b6b67d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.6733\n",
      "Epoch [2/10], Loss: 0.5622\n",
      "Epoch [3/10], Loss: 0.4635\n",
      "Epoch [4/10], Loss: 0.3928\n",
      "Epoch [5/10], Loss: 0.3342\n",
      "Epoch [6/10], Loss: 0.2954\n",
      "Epoch [7/10], Loss: 0.2461\n",
      "Epoch [8/10], Loss: 0.2248\n",
      "Epoch [9/10], Loss: 0.1947\n",
      "Epoch [10/10], Loss: 0.1822\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for X_batch, y_batch in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(X_batch).squeeze()  # Remove extra dimension\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b994dc52",
   "metadata": {},
   "source": [
    "Here's an explanation of each part of this training loop. This code iteratively trains the neural network over a set number of epochs by calculating the loss, performing backpropagation, and updating the model's parameters.\n",
    "\n",
    "### Explanation\n",
    "\n",
    "#### Setting the Number of Epochs:\n",
    "\n",
    "- `num_epochs = 10`: Specifies the number of times the model will iterate over the entire training dataset. In this case, the model will train for 10 epochs.\n",
    "\n",
    "#### Outer Loop (Epoch Loop):\n",
    "\n",
    "- `for epoch in range(num_epochs)`: Loops through each epoch. During each epoch, the model will see every sample in the training dataset once.\n",
    "- `model.train()`: Puts the model into \"training mode.\" In PyTorch, this enables certain behaviors specific to training, such as dropout (if used).\n",
    "\n",
    "#### Tracking Loss:\n",
    "\n",
    "- `running_loss = 0.0`: Initializes a variable to accumulate the total loss for each epoch, which is later used to calculate the average loss for that epoch.\n",
    "\n",
    "#### Inner Loop (Batch Loop):\n",
    "\n",
    "- `for X_batch, y_batch in train_loader`: Loops over each batch of data in the `train_loader`.\n",
    "- Each iteration loads a small subset (batch) of data (`X_batch` and `y_batch`) from the training dataset. This batch size is specified when we defined `train_loader` earlier.\n",
    "\n",
    "#### Forward Pass:\n",
    "\n",
    "- `outputs = model(X_batch).squeeze()`: Passes the batch of inputs (`X_batch`) through the model to obtain predictions (`outputs`).\n",
    "- `.squeeze()`: Removes extra dimensions from `outputs` for compatibility with the loss function, especially if the output has a single dimension (as is common in binary classification).\n",
    "\n",
    "#### Calculate Loss:\n",
    "\n",
    "- `loss = criterion(outputs, y_batch)`: Calculates the difference between the predicted outputs and the actual targets (`y_batch`) using the loss function defined earlier (Binary Cross-Entropy Loss).\n",
    "\n",
    "#### Backward Pass and Optimization:\n",
    "\n",
    "- `optimizer.zero_grad()`: Resets the gradients of all model parameters to zero before backpropagation. This is necessary because, by default, PyTorch accumulates gradients.\n",
    "- `loss.backward()`: Performs backpropagation, calculating the gradients of the loss with respect to each model parameter.\n",
    "- `optimizer.step()`: Updates the model parameters based on the gradients, using the optimization algorithm defined (Adam).\n",
    "\n",
    "#### Accumulate Loss:\n",
    "\n",
    "- `running_loss += loss.item()`: Accumulates the batch loss for calculating the average loss for the epoch. `loss.item()` returns the scalar value of the loss.\n",
    "\n",
    "#### Calculate and Print Average Loss:\n",
    "\n",
    "- `avg_loss = running_loss / len(train_loader)`: Calculates the average loss for the entire epoch by dividing the accumulated loss by the number of batches.\n",
    "- `print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')`: Prints the current epoch and the average loss for that epoch, which is a useful metric to monitor training progress.\n",
    "\n",
    "### Summary\n",
    "\n",
    "The code trains the model for 10 epochs.  \n",
    "In each epoch, the model:\n",
    "- Passes batches of data through the network to compute predictions.\n",
    "- Calculates the loss and updates model parameters to minimize the loss.\n",
    "- Tracks and displays the average loss for each epoch, which is a useful indicator of training progress and model performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7848ba8",
   "metadata": {},
   "source": [
    "\n",
    "## Step 7: Evaluate the Model\n",
    "\n",
    "After training, we evaluate the model's accuracy on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a565bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 96.49%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():  # No need to track gradients\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        outputs = model(X_batch).squeeze()\n",
    "        predicted = (outputs > 0.5).float()  # Convert probabilities to binary labels\n",
    "        total += y_batch.size(0)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "        \n",
    "    accuracy = correct / total\n",
    "    print(f'Test Accuracy: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff451b08",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "\n",
    "#### Setting the Model to Evaluation Mode:\n",
    "\n",
    "- `model.eval()`: Switches the model to evaluation mode.\n",
    "- In PyTorch, this mode disables behaviors specific to training, such as dropout and batch normalization, which helps the model provide stable and reliable predictions on test data.\n",
    "\n",
    "#### Disabling Gradient Calculation:\n",
    "\n",
    "- `with torch.no_grad()`: Disables gradient tracking within this code block.\n",
    "- During evaluation, we don’t need to calculate gradients or update weights, so this saves memory and computation. It also ensures the model only makes predictions.\n",
    "\n",
    "#### Initializing Counters:\n",
    "\n",
    "- `correct = 0` and `total = 0`: These variables count the number of correct predictions and the total number of samples, respectively, to calculate the accuracy.\n",
    "\n",
    "#### Loop Over Test Data:\n",
    "\n",
    "- `for X_batch, y_batch in test_loader`: Iterates over each batch in the `test_loader`.\n",
    "- Each batch contains a subset of test data (`X_batch` with inputs and `y_batch` with labels).\n",
    "\n",
    "#### Forward Pass:\n",
    "\n",
    "- `outputs = model(X_batch).squeeze()`: Passes the batch of inputs through the model to get predictions. The `.squeeze()` removes any extra dimensions in `outputs` to match the labels' dimensions.\n",
    "\n",
    "#### Convert Probabilities to Binary Labels:\n",
    "\n",
    "- `predicted = (outputs > 0.5).float()`: Converts the output probabilities to binary labels.\n",
    "  - Since this is a binary classification task, a threshold of 0.5 is used. Probabilities above 0.5 are classified as 1 (positive class), and those 0.5 or below are classified as 0 (negative class).\n",
    "  - `.float()` converts the binary output to floating-point numbers to match the label format.\n",
    "\n",
    "#### Calculate Accuracy:\n",
    "\n",
    "- `total += y_batch.size(0)`: Increments the total count by the number of samples in the batch.\n",
    "- `correct += (predicted == y_batch).sum().item()`: Compares `predicted` to `y_batch` and sums the number of correct predictions.\n",
    "  - `(predicted == y_batch)` creates a tensor of Boolean values (True for correct predictions and False for incorrect).\n",
    "  - `.sum()` counts the number of `True` values, and `.item()` converts this count to a Python number.\n",
    "\n",
    "#### Compute and Print Accuracy:\n",
    "\n",
    "- `accuracy = correct / total`: Calculates the accuracy by dividing the number of correct predictions by the total number of samples.\n",
    "- `print(f'Test Accuracy: {accuracy * 100:.2f}%')`: Prints the accuracy as a percentage to two decimal places, providing a clear metric of the model's performance on the test set.\n",
    "\n",
    "### Summary\n",
    "\n",
    "This evaluation code:\n",
    "- Switches the model to evaluation mode and turns off gradient tracking to optimize performance.\n",
    "- Iterates over the test dataset in batches, making predictions and calculating accuracy by comparing predicted labels with actual labels.\n",
    "- Prints the final accuracy on the test set, providing a clear performance metric for the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cc877e",
   "metadata": {},
   "source": [
    "\n",
    "## Step 8: Make Predictions\n",
    "\n",
    "Finally, we make a prediction on a sample from the test set to see the model in action.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf690a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Label: 1, Probability: 0.7318\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    # Use a sample from the test set for prediction\n",
    "    sample = X_test_tensor[0].unsqueeze(0)  # Reshape to match model input\n",
    "    prediction = model(sample).item()  # Get probability\n",
    "    label = 1 if prediction > 0.5 else 0  # Convert to binary label\n",
    "    print(f'Predicted Label: {label}, Probability: {prediction:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0bf949",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "\n",
    "#### Disabling Gradient Calculation:\n",
    "\n",
    "- `with torch.no_grad()`: Disables gradient calculation within this block.\n",
    "- Since we’re only making predictions (not training), we don’t need gradients, which reduces memory usage and speeds up computations.\n",
    "\n",
    "#### Selecting and Reshaping a Sample:\n",
    "\n",
    "- `sample = X_test_tensor[0].unsqueeze(0)`: Selects the first sample in the test set (`X_test_tensor[0]`) and reshapes it to add a batch dimension.\n",
    "- `.unsqueeze(0)`: Adds an extra dimension to `sample`, making it compatible with the model’s expected input shape.\n",
    "- This reshaping ensures that the sample is treated as a single batch, matching the model’s input format even when using only one sample.\n",
    "\n",
    "#### Making a Prediction:\n",
    "\n",
    "- `prediction = model(sample).item()`: Passes the sample through the model to generate a prediction.\n",
    "- `model(sample)` outputs a probability value since our model’s output layer uses a sigmoid function.\n",
    "- `.item()`: Extracts this probability as a standard Python number, which is easier to work with.\n",
    "\n",
    "#### Converting the Probability to a Binary Label:\n",
    "\n",
    "- `label = 1 if prediction > 0.5 else 0`: Converts the probability to a binary label based on a threshold of 0.5.\n",
    "- If `prediction` is greater than 0.5, the sample is classified as `1` (e.g., positive class); otherwise, it is classified as `0` (negative class).\n",
    "\n",
    "#### Printing the Result:\n",
    "\n",
    "- `print(f'Predicted Label: {label}, Probability: {prediction:.4f}')`: Displays the predicted label (binary) and the prediction probability (formatted to four decimal places).\n",
    "- This output gives a clear idea of the model’s confidence in its prediction.\n",
    "\n",
    "### Summary\n",
    "\n",
    "- This code selects a single sample from the test set, reshapes it, and uses the model to make a prediction.\n",
    "- The output of the model is interpreted as a probability for binary classification, with probabilities above 0.5 classified as `1` (positive) and others as `0` (negative).\n",
    "- Disabling gradient calculation and reshaping the sample make the prediction efficient and compatible with the model’s input requirements.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
